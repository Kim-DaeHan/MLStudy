{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimJinSang/MLStudy/blob/master/tfidf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXFg4SbJFNhx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "031e4bd2-568d-4be4-9667-68a1ab207894"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.7MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.5.7 (from konlpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/4b/60a3e63d51714d4d7ef1b1efdf84315d118a0a80a5b085bb52a7e2428cdc/JPype1-0.6.3.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 46.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2b/e8/c0b818ac4b3d35104d35e48cdc7afe27fc06ea277feed2831a\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.6.3 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts2_zpAK-qGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOm0YiJJ-1oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KoNLPy의 Okt객체 초기화 ---- ( ※ 1)\n",
        "okt = Okt()\n",
        "# 전역 변수 --- ( ※ 2)\n",
        "word_dic = {'_id': 0} # 단어 사전\n",
        "dt_dic = {} # 문장 전체에서의 단어 출현 횟수\n",
        "files = [] # 문서들을 저장할 리스트\n",
        "\n",
        "def tokenize(text):\n",
        "    '''KoNLPy로 형태소 분석하기''' # --- ( ※ 3) \n",
        "    result = []\n",
        "    word_s = okt.pos(text, norm=True, stem=True)\n",
        "    for n, h in word_s:\n",
        "        if not (h in ['Noun', 'Verb ', 'Adjective']): continue\n",
        "        if h == 'Punctuation' and h2 == 'Number': continue\n",
        "        result.append(n)\n",
        "    return result\n",
        "\n",
        "def words_to_ids(words, auto_add = True):\n",
        "    ''' 단어를 ID로 변환하기 ''' # --- ( ※ 4)\n",
        "    result = []\n",
        "    for w in words:\n",
        "        if w in word_dic:\n",
        "            result.append(word_dic[w])\n",
        "            continue\n",
        "        elif auto_add:\n",
        "            id = word_dic[w] = word_dic['_id']\n",
        "            word_dic['_id'] += 1\n",
        "            result.append(id)\n",
        "    return result\n",
        "\n",
        "def add_text(text):\n",
        "    '''텍스트를 ID 리스트로 변환해서 추가하기''' # --- (*5)\n",
        "    ids = words_to_ids(tokenize(text))\n",
        "    files.append(ids)\n",
        "\n",
        "def add_file(path):\n",
        "    '''텍스트 파일을 학습 전용으로 추가하기''' # --- (*6)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        s = f.read()\n",
        "        add_text(s)\n",
        "\n",
        "def calc_files():\n",
        "    '''추가한 파일 계산하기''' # --- (*7)\n",
        "    global dt_dic\n",
        "    result = []\n",
        "    doc_count = len(files)\n",
        "    dt_dic = {}\n",
        "    # 단어 출현 횟수 세기 --- (*8)\n",
        "    for words in files:\n",
        "        used_word = {}\n",
        "        data = np.zeros(word_dic['_id'])\n",
        "        for id in words:\n",
        "            data[id] += 1\n",
        "            used_word[id] = 1\n",
        "        # 단어 t가 사용되고 있을 경우 dt_dic의 수를 1 더하기 --- (*9)\n",
        "        for id in used_word:\n",
        "            if not(id in dt_dic): dt_dic[id] = 0\n",
        "            dt_dic[id] += 1\n",
        "        # 정규화하기 --- (*10)\n",
        "        data = data / len(words) \n",
        "        result.append(data)\n",
        "    # TF-IDF 계산하기 --- (*11)\n",
        "    for i, doc in enumerate(result):\n",
        "        for id, v in enumerate(doc):\n",
        "            idf = np.log(doc_count / dt_dic[id]) + 1\n",
        "            doc[id] = min([doc[id] * idf, 1.0])\n",
        "        result[i] = doc\n",
        "    return result\n",
        "\n",
        "def save_dic(fname):\n",
        "    '''사전을 파일로 저장하기''' # --- (*12)\n",
        "    pickle.dump(\n",
        "        [word_dic, dt_dic, files],\n",
        "        open(fname, \"wb\"))\n",
        "\n",
        "def load_dic(fname):\n",
        "    '''사전 파일 읽어 들이기''' # --- (*13)\n",
        "    global word_dic, dt_dic, files\n",
        "    n = pickle.load(open(fname, 'rb'))\n",
        "    word_dic, dt_dic, files = n\n",
        "\n",
        "def calc_text(text):\n",
        "    ''' 문장을 벡터로 변환하기 ''' # --- ( ※ 14)\n",
        "    data = np.zeros(word_dic['_id'])\n",
        "    words = words_to_ids(tokenize(text), False)\n",
        "    for w in words:\n",
        "        data[w] += 1\n",
        "    data = data / len(words)\n",
        "    for id, v in enumerate(data):\n",
        "        idf = np.log(len(files) / dt_dic[id]) + 1\n",
        "        data[id] = min([data[id] * idf, 1.0])\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp4DSaR4EZ5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "28f427b4-c3b2-4222-c0f6-bb189957da60"
      },
      "source": [
        "# 모듈 테스트하기 --- ( ※ 15)\n",
        "if __name__ == '__main__':\n",
        "    add_text('비')\n",
        "    add_text('오늘은 비가 내렸어요.') \n",
        "    add_text('오늘은 더웠지만 오후부터 비가 내렸다.') \n",
        "    add_text('비가 내리는 일요일이다.') \n",
        "    print(calc_files())\n",
        "    print(word_dic)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([1., 0., 0., 0., 0.]), array([0.5       , 0.84657359, 0.        , 0.        , 0.        ]), array([0.25      , 0.4232868 , 0.59657359, 0.59657359, 0.        ]), array([0.5, 0. , 0. , 0. , 1. ])]\n",
            "{'_id': 5, '비': 0, '오늘': 1, '덥다': 2, '오후': 3, '일요일': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXsGaakHF2wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}